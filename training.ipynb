{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(example):\n",
    "    return {\n",
    "        'text': f\"Input: {example['Input']}\\nOutput: {example['Output']}\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4764b40318c7407b942b54fd48054b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/857 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(format_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d8ee65793c49bda1b36b8da06b39da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/857 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\suhas\\Documents\\GitHub\\api-documentation-model\\api-doc\\Lib\\site-packages\\transformers\\training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=15, \n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=1500,\n",
    "    weight_decay=0.05,  \n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50, \n",
    "    save_steps=500, \n",
    "    save_total_limit=3, \n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=250, \n",
    "    learning_rate=1e-5, \n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\", \n",
    "    greater_is_better=False, \n",
    "    adam_epsilon=1e-8,  \n",
    "    max_grad_norm=1.0, \n",
    "    lr_scheduler_type=\"cosine\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset.select(range(len(tokenized_dataset) // 5)),  # Increased eval dataset size\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d821ea954eba4eae9ba3967edffa82a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/810 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\suhas\\Documents\\GitHub\\api-documentation-model\\api-doc\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4216, 'grad_norm': 9.520191192626953, 'learning_rate': 3.266666666666667e-07, 'epoch': 0.93}\n",
      "{'loss': 4.2757, 'grad_norm': 8.72707462310791, 'learning_rate': 6.6e-07, 'epoch': 1.85}\n",
      "{'loss': 4.0186, 'grad_norm': 7.514406681060791, 'learning_rate': 9.933333333333333e-07, 'epoch': 2.78}\n",
      "{'loss': 3.6693, 'grad_norm': 6.5963640213012695, 'learning_rate': 1.3266666666666667e-06, 'epoch': 3.7}\n",
      "{'loss': 3.3242, 'grad_norm': 5.176835536956787, 'learning_rate': 1.6600000000000002e-06, 'epoch': 4.63}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9c840b1d064f8ab4904e03fc479dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1443006992340088, 'eval_runtime': 4.2957, 'eval_samples_per_second': 39.808, 'eval_steps_per_second': 5.121, 'epoch': 4.63}\n",
      "{'loss': 2.9698, 'grad_norm': 4.455808162689209, 'learning_rate': 1.9933333333333334e-06, 'epoch': 5.56}\n",
      "{'loss': 2.6162, 'grad_norm': 4.598160266876221, 'learning_rate': 2.3266666666666667e-06, 'epoch': 6.48}\n",
      "{'loss': 2.2454, 'grad_norm': 3.8375515937805176, 'learning_rate': 2.6600000000000004e-06, 'epoch': 7.41}\n",
      "{'loss': 2.0259, 'grad_norm': 4.1292524337768555, 'learning_rate': 2.9933333333333336e-06, 'epoch': 8.33}\n",
      "{'loss': 1.8146, 'grad_norm': 3.4904208183288574, 'learning_rate': 3.326666666666667e-06, 'epoch': 9.26}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0efc432c9c46f89e331017f21593be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5707187056541443, 'eval_runtime': 2.4759, 'eval_samples_per_second': 69.066, 'eval_steps_per_second': 8.886, 'epoch': 9.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\suhas\\Documents\\GitHub\\api-documentation-model\\api-doc\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.683, 'grad_norm': 3.042128086090088, 'learning_rate': 3.66e-06, 'epoch': 10.19}\n",
      "{'loss': 1.5758, 'grad_norm': 2.9849560260772705, 'learning_rate': 3.993333333333334e-06, 'epoch': 11.11}\n",
      "{'loss': 1.5016, 'grad_norm': 2.9033713340759277, 'learning_rate': 4.326666666666667e-06, 'epoch': 12.04}\n",
      "{'loss': 1.4267, 'grad_norm': 3.1110241413116455, 'learning_rate': 4.66e-06, 'epoch': 12.96}\n",
      "{'loss': 1.3662, 'grad_norm': 3.15049147605896, 'learning_rate': 4.986666666666667e-06, 'epoch': 13.89}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0613a27772414a0b90ac2b14986e102f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4433029890060425, 'eval_runtime': 2.4603, 'eval_samples_per_second': 69.503, 'eval_steps_per_second': 8.942, 'epoch': 13.89}\n",
      "{'loss': 1.3042, 'grad_norm': 2.8871264457702637, 'learning_rate': 5.320000000000001e-06, 'epoch': 14.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 718.9969, 'train_samples_per_second': 17.879, 'train_steps_per_second': 1.127, 'train_loss': 2.499399062733591, 'epoch': 15.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=810, training_loss=2.499399062733591, metrics={'train_runtime': 718.9969, 'train_samples_per_second': 17.879, 'train_steps_per_second': 1.127, 'total_flos': 3358909071360000.0, 'train_loss': 2.499399062733591, 'epoch': 15.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_gpt2_api_docs\\\\tokenizer_config.json',\n",
       " './fine_tuned_gpt2_api_docs\\\\special_tokens_map.json',\n",
       " './fine_tuned_gpt2_api_docs\\\\vocab.json',\n",
       " './fine_tuned_gpt2_api_docs\\\\merges.txt',\n",
       " './fine_tuned_gpt2_api_docs\\\\added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./fine_tuned_gpt2_api_docs\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_gpt2_api_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\suhas\\Documents\\GitHub\\api-documentation-model\\api-doc\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "API_Endpoint: https://api.example.com/v1/users\n",
      "API_Method: POST\n",
      "Request_Object:\n",
      "  Request_Header:\n",
      "    Content-Type: application/json\n",
      "    Authorization: Bearer Your_Auth_Token\n",
      "  Request_Body:\n",
      "    username: new_user\n",
      "    email: new_user@example.com\n",
      "    password: secure_password123\n",
      "Response_body: Â {\n",
      "\"user_id\": \"new_username\",\n",
      "}\n",
      "Output: {\"user\": {},\n",
      "{ \"email\": {\"email_address\": \"+email@gmail.co.uk\"}}, \"password\": \"[email protected]\"\n",
      "EndPoint: http://example-api-1.amazonaws.net/api/1\n",
      "Sample Response: { \"user1\": [{\"email\":\"new-user\", \"name\":\"John Doe\",}}]\n",
      "Example Response Body: [{}, {{\"user2\": [\"John\", {\"name\": 'John', \"age\": 25}, {\"age\":\"25\"}], \"response_data\":{}] }\n",
      "Step 3: Create a new instance of the API\n",
      "Create a New App\\User\\Auth\\Example\\Sample\\Server\\Client\\Instance\\example_api_1\\user.json file with the following content: Content: \"{\\\"user\\\": {\\\"email\\\": \\\"new\\\", \\\"name\\\": 'New User', \\\"age\\\": 25}\", 'response\\\": {\"username\": \\\"New user', 'email': \\\"email-address-new@google.ca.us', ''password\\\": ''secure_passphrase123\"}]\", // Create the server instance using the provided credentials. // The following code will create a server for the example app. use Illuminate\\Http\\Request\\Parameters\\{HttpRequestBuilder\\}\\connection = HttpRequest.newBuilder();\\nHttpClient client = new HHttpBuilder().newHttp(); client.connect(connection, HContext.HTTP_CURL, Connection.GET_APIURL());\\r\\t\\tservice.post('/user', function (req, res) { console.log(req.body.data); });\\w\\d\\ttag 'Server: 'localhost:3000'\\u003d' -> server.get('localhost', 5000);\\c\\\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_path = \"./fine_tuned_gpt2_api_docs\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "def generate_api_doc(input_text, max_length=500):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2, temperature=0.7)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "input_text = \"\"\"\n",
    "API_Endpoint: https://api.example.com/v1/users\n",
    "API_Method: POST\n",
    "Request_Object:\n",
    "  Request_Header:\n",
    "    Content-Type: application/json\n",
    "    Authorization: Bearer Your_Auth_Token\n",
    "  Request_Body:\n",
    "    username: new_user\n",
    "    email: new_user@example.com\n",
    "    password: secure_password123\n",
    "\"\"\"\n",
    "\n",
    "generated_doc = generate_api_doc(input_text)\n",
    "print(generated_doc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "api-doc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
